{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fd7de407",
   "metadata": {},
   "source": [
    "# Train / Test 데이터 세트 분할 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41c6a478",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## 1. 과적합(Overfit) 과 과소적합(Underfit)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "940cb8bc",
   "metadata": {},
   "source": [
    "### Overfitting (과적합)\n",
    "> 훈련 데이터에 너무 잘 맞는 모델이 만들어지는 현상\n",
    "- Training set에 대한 성능이 좋음\n",
    "- Test set에 대한 성능이 떨어짐\n",
    "- High Variance (높은 분산) Model\n",
    "\n",
    "\n",
    "### Underfitting (과소적합)\n",
    "> 모델이 훈련 데이터에 충분히 학습되지 않은 현상\n",
    "- 모델이 너무 단순해서 데이터의 내재된 패턴을 학습하지 못함 \n",
    "- High Bias (높은 편향) Model\n",
    "\n",
    "\n",
    "### Bias-Variance Tradeoff (편향-분산 균형)\n",
    "> 모델의 복잡성과 일반화 능력 사이의 균형\n",
    "\n",
    "- Bias (편향): 모델이 실제 데이터의 패턴을 얼마나 잘 학습하는지를 나타내는 지표\n",
    "- Variance (분산): 모델이 훈련 데이터에 얼마나 민감하게 반응하는지를 나타내는 지표\n",
    "\n",
    "#### ML의 3가지 Error Source\n",
    "1. 학습데이터와 실제 데이터 분포에 의한 Error (Variance)\n",
    "2.가설 모델과 실제 기능의 차이에 대한 Error (Bias)\n",
    "3. 데이터의 노이즈에 의한 Error (제거 불가 에러)\n",
    "\n",
    "#### Tradeoff (Bias-Variance Dliemma)\n",
    "- 분산 감소 -> 데이터셋 크기 증가 \n",
    "- 편향 감소 -> 모델 복잡도 증가"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bc20fa8",
   "metadata": {},
   "source": [
    "## 2. 데이터셋\n",
    "\n",
    "### Training, Testing, Cross-Validation set \n",
    "#### Training set\n",
    "- 모델 학습을 위해 사용하는 데이터 \n",
    "- parameter를 조정하는 과정에서 사용하는 데이터 \n",
    "- 보지 못한 데이터의 분포가 Training set과 다를 수 있기 때문에, 모델이 Training set에 과적합(overfitting)되지 않도록 주의\n",
    "- Training set 내에서 cross-validation set을 만들어 모델의 성능을 평가하는 데 사용\n",
    "\n",
    "\n",
    "#### Testing set\n",
    "- 모델의 최종 성능을 평가하기 위해 사용하는 데이터\n",
    "- 학습한 모델을 테스트하기 위해 사용되는 데이터 \n",
    "- Training과 Testing은 서로 섞이지 않고, `동일한 분포`를 가져야함 \n",
    "\n",
    "#### Cross-Validation set\n",
    "- 교차검증 세트 \n",
    "- 훈련세트를 여러 개의 작은 세트로 나누어 모델을 평가하는 방법\n",
    "- 모델의 일반화 성능을 평가하기 위해 사용\n",
    "- 모델의 하이퍼파라미터 튜닝에 사용\n",
    "- 데이터의 수가 적은 경우 사용 가능 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bf0e98f",
   "metadata": {},
   "source": [
    "## 3. 지도학습 모델의 분류 성능 평가 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3016d10e",
   "metadata": {},
   "source": [
    "## 혼돈 행렬\n",
    "\n",
    "|구분|실제 Positive|실제 Negative|\n",
    "|---|---|---|\n",
    "|예측 Positive|TP|FP(Type 1 Error)|\n",
    "|예측 Negative|FN(Type 2 Error)|TN|\n",
    "\n",
    "\n",
    "## 단순 정확도 \n",
    "- 분류 성능의 정확성을 측정 \n",
    "$$\\textrm{Classification rate(정확도)}=\\frac{\\textrm{(TP+TN)}}{\\textrm{(TP+TN+FP+FN)}} $$\n",
    "\n",
    "\n",
    "## Precision (정밀성) \n",
    "> 모델이 샘플을 True로 얼마나 자주 분류하였는가? \n",
    "- Positive 분류의 정확성 측정 \n",
    "- Positive 중에서 True Positive인 비율\n",
    "- 1에 가까울수록 좋음 \n",
    "$$\\textrm{Precision(정밀성)}=\\frac{\\textrm{(TP)}}{\\textrm{(TP+FP)}} $$\n",
    "\n",
    "## Recall (민감도)\n",
    "- 전체 Positive 데이터 중에서 Positive 로 분류한 비율 \n",
    "- 1에 가까울수록 좋음 \n",
    "- Positive case를 놓치고 싶지 않은 경우의 성능 측정 \n",
    "  - `Type2 Error` 측정 \n",
    "$$\\textrm{Recall(민감도)}=\\frac{\\textrm{(TP)}}{\\textrm{(TP+FN)}} $$\n",
    "\n",
    "\n",
    "## Precision/Recall Trade off (정밀성/민감도 트레이드오프)\n",
    "- 정확히 검출이 되기 원하는 경우 : Precision (정밀성) 상승 \n",
    "- 누락되는 검출이 없도록 하는 경우 : Recall(민감도) 상승 \n",
    "\n",
    "### 전체적 성능 측정에 활용 (조화 평균)\n",
    "$$\\textrm{F1-Score)}=\\frac{\\textrm{(정밀도 * 민감도)}}{\\textrm{(정밀도+민감도)}} $$\n",
    "- 1에 가까울 수록 성능이 좋음\n",
    "- 0에 가까울 수록 성능이 좋지 않음 \n",
    "\n",
    "### ROC Curve\n",
    "- 수신자 조작 특성 곡선 \n",
    "- 분류기간의 성능을 비교하기 위한 곡선 \n",
    "- `FPR` : Negative를 Positive로 잘못 분류 \n",
    "- `TPR` : Positive를 Negative로 잘못 분류 \n",
    "- AUC에 있으면 좋은 상황이라고 함\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mldl_tutorial (3.12.11)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
