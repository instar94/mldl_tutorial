{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3e8d3677",
   "metadata": {},
   "source": [
    "# Ensemble Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e84f9d2f",
   "metadata": {},
   "source": [
    "## 1. 앙상블 학습이란?\n",
    "> 다수의 `약한 학습기`를 조합하여 더욱 높은 성능 추출 \n",
    "- `약한 학습기` : 단일 모델보다 약간 더 나은 성능을 보이는 모델\n",
    "\n",
    "### 학습의 종류 \n",
    "- `배깅(Bagging)` : 여러 모델을 병렬로 학습하여 예측 결과를 결합 (ex. 랜덤 포레스트) -> 분산 감소\n",
    "- `부스팅(Boosting)` : 여러 모델을 순차적으로 학습하여 이전 모델의 오류를 보완 (ex. 그래디언트 부스팅, XGBoost) -> 편향 감소"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36023636",
   "metadata": {},
   "source": [
    "### 배깅 \n",
    "- `복원 추출` : 중복 Sampling을 통해 여러 개의 훈련 데이터셋 생성 \n",
    "- b개의 독립 학습 셋 평균시 분산 $\\frac{\\sigma^2}{b}$ 감소 하면서 평균은 동일 \n",
    "- `Aggregating` : 여러 모델의 예측 결과 중 투표하여 최종 예측 생성\n",
    "- 대표 예시 : `Random Forest` (여러 개의 결정 트리를 사용하여 예측을 수행하는 앙상블 모델)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "500c68dc",
   "metadata": {},
   "source": [
    "#### Bagging Tree 알고리즘 \n",
    "- 훈련셋에서 랜덤 샘플링 -> $X_b, y_b$ 선택 \n",
    "- $X_b, y_b$ 로 `ID3` 알고리즘으로 결정트리 구성 \n",
    "- b개의 트리 생성 \n",
    "- b개의 모든 트리를 이용하여 분류 진행 \n",
    "- `majority voting` : b개의 트리에서 가장 많이 예측된 클래스를 최종 예측으로 선택\n",
    "- 편향을 유지하면서 분산 감소 가능 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94864f5e",
   "metadata": {},
   "source": [
    "#### Random Forest 알고리즘\n",
    "- Bagging Tree에서 발전 \n",
    "- `bootstrap 샘플링` + `특성 무작위 선택`을 통한 트리 생성 \n",
    "- Tree base 모델 -> white box 모델 특성 유지 \n",
    "- 예측 정확도가 높음 \n",
    "- 병렬적으로 생성 가능 및 빠른 예측 속도 \n",
    "- 각 트리의 깊이가 깊게 생성 \n",
    "  - 인위적으로 prune 을 하지 않으나 앙상블하면 낮은 편형과 낮은 분산 값을 얻음"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "978fb467",
   "metadata": {},
   "source": [
    "### Boosting\n",
    "> 미분류된 데이터에 더 높은 가중치를 부여하여 다음 번 모델의 Sampling에 포함될 확률을 높이는 방법\n",
    "\n",
    "- 다수의 약한 학습자를 훈련시켜 majority voting하는 앙상블 기법 \n",
    "- 예시 : `AdaBoost`(Adaptive Boosting), `Gradient Boosting`, `XGBoost` 등"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4ee1dab",
   "metadata": {},
   "source": [
    "#### Gradient Bootsting 알고리즘 \n",
    "- 약한 학습자 (Weak Learner) : 랜덤 선택보다 약간 더 나은 성능의 모델 -> 결정 트리 사용 \n",
    "- 이전 트리에서 발생한 `잔차(residual error)`를 다음 트리에서 보정\n",
    "- 기존의 약한 학습자가 생성한 `잔차(residual error)`를 감소시키는 추가 트리를 더 이상 감소효과가 없을때 까지 생성 \n",
    "- Gradient Descent(경사하강법)에 의해 손실함수(예시 : MSE)를 최적화 \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce7d0dfa",
   "metadata": {},
   "source": [
    "#### AdaBoost\n",
    "- `AdaBoost` : 여러개의 `Stump`을 조합하여 강한 학습자 생성\n",
    "- `Stump` : 약한 학습자`들`  (기존의 약한학습자 단점 보완) \n",
    "- 이전 stump에서 잘못 분류한 데이터가 다음 샘플링에 포함되도록 가중치를 높음 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f6820e8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mldl_tutorial (3.12.11)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
